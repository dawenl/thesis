%!TEX root = thesis.tex

\chapter{Introduction}\label{chpt:intro}

\section{Motivation}\label{chpt:intro:sec:motivation}

Understanding complex real-world data is crucial as we are overwhelmed with the massive amount of information that is constantly generated. For example, there are hundreds or even thousands of new songs being released every day. Ideally we would like to get personalized spot-on recommendation without browsing through every single one of them. This requires understanding both the music semantics (``Is this song a standard upbeat pop tune or a 20-minute multi-sectional epic progressive rock masterpiece?'') from acoustic waveform, as well as our music preferences, which can be so subtle that sometimes even we cannot describe precisely ourselves.  

As another example, we consider arXiv.org\footnote{\url{http://arxiv.org} is a pre-print repository
for scientific papers.} where scholars upload the latest scientist discovery everyday. For a field that is rapidly developing, like machine learning, there can be tens or even hundreds of new papers being uploaded each day. Even though the papers on arXiv.org are categorized by subject, it is still impractical to even skim through every new paper in the subject of our interests. On the other hand, we also do not want to miss the latest scientific progress that is happening in our field. Ideally we would like to get personalized recommendations from the massive amount of new papers. Similar to music recommendation, this requires understanding both the content of the paper (``Is this paper about unsupervised learning or reinforcement learning?'') from the text, as well as our preferences/fields of interest. 

In this dissertation, we aim to understand the complex real-world data (music, articles, and user feedback) by applying the tools of Bayesian probabilistic models, or more precisely, latent variable models. We give a high-level introduction to probabilistic latent variable models below, and put everything into the context of music and paper recommendation examples outlined above. 

\section{Probabilistic latent variable models}\label{chpt:intro:sec:model}

The basic idea behind probabilistic latent variable models is that we assume there exists some process that stochastically generates the data we observe. We further assume that the data-generating process is governed by some latent structures. As a concrete example, let's consider latent Dirichlet allocation model \citep{blei2003latent}, a widely used probabilistic latent variable model for documents. The model assumption is that when generating a document, we first select a latent \textit{topic} (e.g., business, sports, or politics), then select a word that often appears in the selected \textit{topic} (e.g., the word ``election'' will commonly appear in a topic about politics), and repeat this process for every word. Here we do not observe the latent structures (topics). However, when we make such assumption and fit the model with text data, we are able to discover the latent topics which help us organize, browse, and retrieve large text corpus more easily and efficiently. 

As mentioned above, when we design a model, we make certain assumptions about the latent structures and data-generating process. The data-generating process does not have to be absolutely correct. (In fact, they are never correct, as George E. P. Box once put it: ``All models are wrong, but some are useful.'') Related to both music and paper recommendation examples, a commonly used probabilistic model for recommendation is matrix factorization (details in \Cref{chpt:background:sec:mf_cf}). The general model assumption of matrix factorization for recommendation is that a user's feedback towards an item is generated by the combination of two latent variables: user preference and item attribute. The item attribute can itself be generated from a prior, or generated from a probabilistic model of the actual item content (acoustic waveform or document text). This seems to be an overly-simplified data-generating process. However, it proves effective in many scenarios and is widely used in commercial recommender systems. 

When we fit the model via posterior inference (we give details about general inference procedure in \Cref{chpt:background:sec:inference}), we uncover these latent structures. These latent structures reveal interesting aspects of the data, e.g., we know for some users, they enjoy classical music and papers about Bayesian statistics. Concretely, in \Cref{chpt:tagging}, we fit a latent variable model to a music collection of 370k tracks to predict music semantic tags (e.g., genre, instrumentation, mood, etc.) from acoustic features. By exploring the model in \Cref{tab:factors}, we can get an idea of what portion of the acoustic space is being captured by the latent variables, and whether it is musically coherent. In \Cref{chpt:expomf}, we introduce the notation of \textit{user exposure} (whether a user is exposed to an item or not) in recommender systems as a latent variable. After fitting the model, we are able to reason about if the model believes a user has been exposed to certain items in \Cref{chpt:expomf:fig:expo_exp} and \Cref{chpt:expomf:fig:si}. These exploratory study can help us better understand the complex data at hand and provide insight into what the model is capturing. 

\section{Contributions}\label{chpt:intro:sec:contribution}
Below we outline the contributions of this dissertation. Since we address a number of different problems in subsequent chapters, we provide more thorough literature reviews of specific prior work in each chapter.

\subsection{Music understanding and recommendation}
\parhead{Scalable music tagging with Poisson factorization.} 
We develop scalable solution to automatic music tagging -- inferring the semantic tags from the audio features. We treat music tagging as a matrix completion problem and apply the Poisson factorization model to a large collection of music data. We explore the fitted model and identify what portion of the acoustic codeword space is being captured by the latent variables.

\parhead{Content-aware collaborative music recommendation.} 
We address the fundamental cold-start problem (it cannot recommend new songs that no one has listened to) of collaborative filtering by pre-training a multi-layered neural network on semantic tagging information as a content model and using it as a prior in a collaborative filtering model.  The proposed system shows comparably better result than the state-of-the-art collaborative filtering approaches, in addition to the favorable performance in the cold-start case. 

\subsection{Probabilistic models for recommender systems}
\parhead{Modeling user exposure in recommendation.}
We develop a probabilistic matrix factorization model to capture the latent user exposure (whether or not a user is exposed to an item). In doing so, we recover one of the most successful state-of-the-art approaches as a special case of our model
\citep{hu2008collaborative}, and provide a plug-in method for conditioning
exposure on various forms of exposure covariates (e.g., topics in text,
venue locations). In four datasets from various domains, we show that our model
outperforms existing benchmarks both with and
without exposure covariates. 

\parhead{Causal inference for recommendation.}
We develop a causal inference approach to recommender systems. 
We use inverse propensity weighting to correct for the bias which exists in observational recommendation data. Through extensive empirical study, we demonstrate that this causal approach to recommender systems leads to improved generalization to new data.


\section{Related publications}

The work presented in this dissertation is largely based on published articles in various conference proceedings: \Cref{chpt:tagging} and \Cref{chpt:content} are based on papers presented in ISMIR 2014 \citep{liang2014codebook} and ISMIR 2015 \citep{liang15content}, respectively. \Cref{chpt:expomf} is based on our paper presented in WWW 2016 \citep{Liang16exposure}. \Cref{chpt:causal_rec} is based on our paper which is currently in submission \citep{liang16causal}.
